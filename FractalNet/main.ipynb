{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is implementation of [FractalNet](http://arxiv.org/abs/1605.07648) paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-06-07T14:35:50.118954",
     "start_time": "2016-06-07T14:35:38.888515"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import time\n",
    "\n",
    "height, width, channels = 32, 32, 3\n",
    "noClasses = 10\n",
    "batch_size = 50\n",
    "\n",
    "logdir = \"/tmp/FractalNet/test1\"\n",
    "test_log_steps = 10\n",
    "\n",
    "cifar10root = os.path.expanduser('~/Downloads/cifar-10-batches-py')\n",
    "cifar10root\n",
    "\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "\n",
    "for i in range(1, 6):\n",
    "    with open(os.path.join(cifar10root, \"data_batch_%d\" %i), 'rb') as f:\n",
    "        data_batch = pickle.load(f, encoding='latin1')\n",
    "        train_data.append(data_batch['data'])\n",
    "        train_labels.append(np.array(data_batch['labels']).reshape(-1,1))\n",
    "\n",
    "train_data = np.vstack(train_data)\n",
    "train_labels = np.vstack(train_labels)\n",
    "train_labels = OneHotEncoder(noClasses, sparse=False).fit_transform(train_labels)\n",
    "\n",
    "with open(os.path.join(cifar10root, \"test_batch\"), 'rb') as f:\n",
    "    test = pickle.load(f, encoding='latin1')\n",
    "    test_data = test['data']\n",
    "    test_label = np.array(test['labels']).reshape(-1,1)\n",
    "    test_label = OneHotEncoder(noClasses, sparse=False).fit_transform(test_label)\n",
    "    \n",
    "scale = StandardScaler().fit(train_data)\n",
    "train_data = scale.transform(train_data).reshape(-1, height, width, channels)\n",
    "test_data  = scale.transform(test_data).reshape(-1, height, width, channels)\n",
    "\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-06-07T14:35:53.902690",
     "start_time": "2016-06-07T14:35:50.123141"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 not supported (please install/reinstall h5py)\n",
      "0 (?, 16, 16, 16)\n",
      "1 (?, 8, 8, 32)\n",
      "2 (?, 4, 4, 64)\n",
      "3 (?, 2, 2, 128)\n",
      "4 (?, 1, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn    \n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "\n",
    "class FractalNet():\n",
    "    def __init__(self, input, n, f_height = 2, f_width = 2, out_chanell=None):\n",
    "        _, _, _, c = input.get_shape()\n",
    "        in_channel = int(c)\n",
    "        if out_chanell is None:\n",
    "            out_chanell = in_channel\n",
    "            \n",
    "        self.n = n \n",
    "        self.children = []\n",
    "        with tf.name_scope(\"F%d\" % n):\n",
    "            with tf.name_scope(\"atom\"):\n",
    "                # single comutational \"atom\" in FractalNet\n",
    "                \n",
    "                self.filter = tf.Variable(tf.truncated_normal(\n",
    "                        [f_height, f_width, in_channel, out_chanell], \n",
    "                        stddev=0.35\n",
    "                    ),\n",
    "                    name=\"filter\"\n",
    "                )\n",
    "                \n",
    "                self.bias = tf.Variable([0]*out_chanell, dtype=tf.float32, name=\"bias\")\n",
    "                atom = tf.nn.conv2d(input, self.filter, [1,1,1,1], 'SAME')\n",
    "                atom = tf.nn.relu(tf.nn.bias_add(atom, self.bias))\n",
    "                atom = batch_normalization(atom)\n",
    "                \n",
    "            self.__tensors = [atom]\n",
    "            if n > 1:\n",
    "                Fp = FractalNet(input, n - 1, f_height, f_width, int((out_chanell + in_channel)/2))\n",
    "                self.children.append(Fp)\n",
    "                Fp = FractalNet(Fp.get_tensor(), n - 1, f_height, f_width, out_chanell)\n",
    "                self.children.append(Fp)\n",
    "                self.__tensors.extend(Fp.__tensors)\n",
    "            \n",
    "            with tf.name_scope(\"join\"):\n",
    "                # activations in join layer \n",
    "                # for mean join layer they should be equal and sum to 1\n",
    "                self.is_active = [\n",
    "                    tf.Variable(1.0/n, trainable=False, name=\"a%d\"%i)\n",
    "                    for i in range(n)\n",
    "                ]\n",
    "            \n",
    "                self.__tensor = tf.add_n(\n",
    "                    [tf.mul(m, x) for m, x in zip(self.is_active, self.__tensors)], \n",
    "                    name=\"Average_pool_join\"\n",
    "                )\n",
    "\n",
    "    def get_tensor(self):\n",
    "        return self.__tensor\n",
    "\n",
    "    def genAssignJoinValues(self, values):\n",
    "        return tf.group(*[\n",
    "            var.assign(val)\n",
    "            for val, var in zip(values, self.is_active)\n",
    "        ])\n",
    "\n",
    "    def genColumn(self, column):\n",
    "        assert 0 <= column < self.n\n",
    "\n",
    "        values = np.zeros(len(self.is_active))\n",
    "        values[column] = 1\n",
    "        \n",
    "        return tf.group(\n",
    "            self.genAssignJoinValues(values),\n",
    "            *[fp.genColumn(column - 1) for fp in self.children if column > 0]\n",
    "        )\n",
    "\n",
    "    def genRandomColumn(self):\n",
    "        return self.genColumn(np.random.randint(self.n))\n",
    "        \n",
    "    \n",
    "    def genLocalDropPath(self, dropout_prob):\n",
    "        values = np.zeros(self.n)\n",
    "        while np.sum(values) < 0.5: # ==0; floating point correction\n",
    "            values = (np.random.random(self.n) > dropout_prob).astype(np.float32)\n",
    "        values /= np.sum(values) #normalize sum to 1\n",
    "        return tf.group(\n",
    "            self.genAssignJoinValues(values),\n",
    "            *[fp.genLocalDropPath(dropout_prob) for fp in self.children]\n",
    "        )\n",
    "    \n",
    "    def genTestMode(self):\n",
    "        \"\"\"\n",
    "            Kills any droppaths set\n",
    "        \"\"\"\n",
    "        return self.genAssignJoinValues(np.ones(self.n, dtype=np.float32)/self.n)\n",
    "    \n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    X = tf.placeholder(tf.float32, [None, height, width, channels], name=\"input\")\n",
    "    Y = tf.placeholder(tf.float32, [None, noClasses], name=\"labels\")\n",
    "    FF = []\n",
    "    net = X\n",
    "    \n",
    "    for i, channel_no in enumerate([16, 32, 64, 128, 128]):\n",
    "        with tf.name_scope(\"block_%d\" % (i + 1)):\n",
    "            net = FractalNet(net, 1, out_chanell=channel_no)\n",
    "        FF.append(net)\n",
    "        net = tf.nn.max_pool(net.get_tensor(), [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "        print(i, net.get_shape())\n",
    "    \n",
    "    net = tflearn.fully_connected(net, noClasses)\n",
    "    yp = tf.nn.softmax(net)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n",
    "    learning_rate = tf.Variable(0.01, trainable=False)\n",
    "    halve_lr = learning_rate.assign(tf.mul(learning_rate, 0.5))\n",
    "    train = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss)\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.nn.in_top_k(yp, tf.argmax(Y, 1), k = 1), tf.float32)\n",
    "    )\n",
    "    \n",
    "    learning_rate_summ = tf.scalar_summary(\"Learning rate\", learning_rate)\n",
    "    train_acc_summ = tf.scalar_summary(\"Train accuracy\", acc)\n",
    "    train_loss_summ = tf.scalar_summary(\"Train loss\", loss)\n",
    "    \n",
    "    \n",
    "    train_summ = tf.merge_summary(\n",
    "        [learning_rate_summ, train_acc_summ, train_loss_summ], \n",
    "        name=\"Training_summary\"\n",
    "    )\n",
    "    \n",
    "    test_acc_summ = tf.scalar_summary(\"Test accuracy\", acc)\n",
    "    test_loss_summ = tf.scalar_summary(\"Test loss\", loss)\n",
    "    \n",
    "    test_summ = tf.merge_summary(\n",
    "        [test_acc_summ, test_loss_summ],\n",
    "        name=\"Test_summary\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-06-07T14:35:57.748477",
     "start_time": "2016-06-07T14:35:53.914784"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5a732a341293>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m             summ, _ = sess.run([train_summ, train], feed_dict={\n\u001b[0;32m     20\u001b[0m                     \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                     \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                 })\n\u001b[0;32m     23\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msumm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 564\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 637\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    638\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    642\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m--> 628\u001b[1;33m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter(logdir, sess.graph)\n",
    "    step = 0\n",
    "    reduce_learning = [200, 300, 350, 375]\n",
    "    for epoh in range(400):\n",
    "        t = time.clock()\n",
    "        if epoh in reduce_learning:\n",
    "            sess.run(halve_lr)\n",
    "        tflearn.is_training(True)\n",
    "        batch_idxs = np.random.permutation(train_data.shape[0])\n",
    "        for batch in np.split(batch_idxs, train_data.shape[0]/batch_size):\n",
    "            step += 1\n",
    "            for F in FF[::2]:\n",
    "                sess.run(F.genLocalDropPath(0.15))\n",
    "            for F in FF[1::2]:\n",
    "                sess.run(F.genRandomColumn())\n",
    "            summ, _ = sess.run([train_summ, train], feed_dict={\n",
    "                    X: train_data[batch],\n",
    "                    Y: train_labels[batch]\n",
    "                })\n",
    "            writer.add_summary(summ, step)\n",
    "            if step % test_log_steps == 0 or step == 1:\n",
    "                tn = time.clock()\n",
    "\n",
    "                tflearn.is_training(False)\n",
    "                for F in FF:\n",
    "                    sess.run(F.genTestMode())\n",
    "                summ, top1, avg_loss = sess.run([test_summ, acc, loss], feed_dict={\n",
    "                        X: test_data,\n",
    "                        Y: test_label\n",
    "                    })\n",
    "                writer.add_summary(summ, step)\n",
    "                print(\"[{:10d}] step, acc: {:2.5f}%, loss: {:2.7f} steps/sec{:5.2f}\"\n",
    "                      .format(step, 100 * top1, avg_loss, test_log_steps/(tn-t)))\n",
    "                t = tn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
