{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pylab import *\n",
    "from scipy.special import expit\n",
    "\n",
    "class sigmoid():\n",
    "    @staticmethod\n",
    "    def f(x):\n",
    "        return expit(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def df(x):\n",
    "        return sigmoid.f(x) * (1 - sigmoid.f(x))\n",
    "\n",
    "class crossEntropy():\n",
    "    @staticmethod\n",
    "    def f(y, a):\n",
    "        \"\"\"\n",
    "            y -> expected\n",
    "            a -> actual\n",
    "        \"\"\"\n",
    "        assert a.shape == y.shape\n",
    "        error = -np.sum(y*np.log(a) + (1 - y)*np.log(1 - a), axis = 1)\n",
    "        assert error.shape[0] == a.shape[0]\n",
    "        return np.average(error, axis = 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def df(y, a):\n",
    "        assert a.shape == y.shape\n",
    "        return (a - y) / (a * (1-a) * a.shape[0])\n",
    "\n",
    "class sqrcost():\n",
    "    \n",
    "    @staticmethod\n",
    "    def f(y, a):\n",
    "        \"\"\"\n",
    "            y -> expected\n",
    "            a -> actual\n",
    "        \"\"\"\n",
    "        assert a.shape == y.shape\n",
    "        # averaging over all test cases\n",
    "        return 0.5 * np.sum(np.average(np.square(y - a), axis = 0))\n",
    "    \n",
    "    @staticmethod\n",
    "    def df(y, a):\n",
    "        assert a.shape == y.shape\n",
    "        return (a - y) / a.shape[0] # number or test cases to average them out\n",
    "    \n",
    "class NN():\n",
    "    \"\"\"\n",
    "        struct => (2, 1)\n",
    "        number of units in each layer\n",
    "    \"\"\"\n",
    "    def __init__(self, struct, cost, sigma):\n",
    "        self.N = len(struct)\n",
    "        self.struct = struct\n",
    "        \n",
    "        self.b = [np.ones(shape=(1,x), dtype=np.float64) for x in struct[1:]]\n",
    "        self.w = [np.random.randn(n, m) / np.sqrt(n) for n,m in zip(struct, struct[1:])]\n",
    "\n",
    "        self.cost = cost\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def ff(self, x):\n",
    "        self.a = [x]\n",
    "        self.z = []\n",
    "        for w, b in zip(self.w, self.b):\n",
    "            x = np.dot(x, w) + b\n",
    "            self.z.append(x)\n",
    "            x = self.sigma.f(x)\n",
    "            self.a.append(x)\n",
    "        return x\n",
    "    \n",
    "    def grad(self, x, y):\n",
    "        self.ff(x)\n",
    "        self.d = [0 for _ in range(self.N - 1)]\n",
    "        self.d[-1] = self.cost.df(y, self.a[-1]) * self.sigma.df(self.z[-1])\n",
    "        for l in range(2, self.N):\n",
    "            self.d[-l] = np.dot(self.d[-l + 1], self.w[-l + 1].T) * self.sigma.df(self.z[-l])\n",
    "            \n",
    "        wgrad = []\n",
    "        bgrad = []\n",
    "        for i in range(self.N - 1):\n",
    "            # summing all errors\n",
    "            # They are already averaged in cost function\n",
    "            bgrad.append(np.sum(self.d[i], axis = 0)) \n",
    "            wgrad.append(np.dot(self.a[i].T,self.d[i]))\n",
    "        \n",
    "        return (wgrad, bgrad)\n",
    "    \n",
    "    def train(self, x, y, minibatch = 10, learn = 0.001):\n",
    "        assert x.shape[1] == self.struct[0]\n",
    "        assert y.shape[1] == self.struct[-1]\n",
    "        assert x.shape[0] == y.shape[0]\n",
    "        \n",
    "        sz = self.struct[0]\n",
    "        joinedData = np.hstack((x, y))\n",
    "        self.trainJoinedData(joinedData, sz, minibatch=minibatch, learn=learn)\n",
    "        \n",
    "    def trainJoinedData(self, data, separatingColumn, minibatch, learn):\n",
    "        np.random.shuffle(data)\n",
    "        sz = separatingColumn\n",
    "        for batch in np.split(data, minibatch):\n",
    "            (gw, gb) = self.grad(batch[:, :sz], batch[:, sz:])\n",
    "            for w, dw in zip(self.w, gw):\n",
    "                w -= learn * dw\n",
    "    \n",
    "            for b, db in zip(self.b, gb):\n",
    "                b -= learn * db\n",
    "            \n",
    "        \n",
    "    def avgCost(self, x, y):\n",
    "        return self.cost.f(y, self.ff(x))\n",
    "    \n",
    "    def ngrad(self, x, y):\n",
    "        eps = 1e-4\n",
    "        wgrad = list(map(np.zeros_like, self.w))\n",
    "        bgrad = list(map(np.zeros_like, self.b))\n",
    "        \n",
    "        for i in range(len(self.w)):\n",
    "            rr, cc = self.w[i].shape\n",
    "            for r in range(rr):\n",
    "                for c in range(cc):\n",
    "                    o = self.w[i][r,c]\n",
    "                    self.w[i][r,c] = o + eps\n",
    "                    uc = self.avgCost(x, y)\n",
    "                    self.w[i][r,c] = o - eps\n",
    "                    lc = self.avgCost(x, y)\n",
    "                    self.w[i][r,c] = o\n",
    "\n",
    "                    wgrad[i][r,c] = (uc - lc) / (2*eps)\n",
    "    \n",
    "        for i in range(len(self.b)):\n",
    "            rr, cc = self.b[i].shape\n",
    "            for r in range(rr):\n",
    "                for c in range(cc):\n",
    "                    o = self.b[i][r,c]\n",
    "                    self.b[i][r,c] = o + eps\n",
    "                    uc = self.avgCost(x, y)\n",
    "                    \n",
    "                    self.b[i][r,c] = o - eps\n",
    "                    lc = self.avgCost(x, y)    \n",
    "                    self.b[i][r,c] = o\n",
    "\n",
    "                    bgrad[i][r,c] = (uc - lc) / (2*eps)\n",
    "        \n",
    "        return (wgrad, bgrad)\n",
    "    \n",
    "nn = NN((1,3,3), crossEntropy, sigmoid)\n",
    "\n",
    "x_train = np.array([[1], [0]])\n",
    "y_train = np.array([[0,0,0], [1,0, 0]])\n",
    "\n",
    "np.set_printoptions(precision = 10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max w diff:  1.39480066386e-10\n",
      "max b diff:  1.57874935347e-10\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "print (nn.ff(x_train[[0], :]))\n",
    "print (\"=== z & a ===\")\n",
    "print (nn.z)\n",
    "print (nn.a)\n",
    "\n",
    "print (\"=== numerical gradient ===\")\n",
    "nw, nb = nn.ngrad(x_train[[0], :], y_train[[0], :])\n",
    "print (nw[0].shape, nw)\n",
    "print(nb[0].shape, nb)\n",
    "\n",
    "print (\"=== backprop gradient ===\")\n",
    "w, b = nn.grad(x_train[[0], :], y_train[[0], :])\n",
    "print (w[0].shape, w)\n",
    "print(b[0].shape, b)\n",
    "\n",
    "print (\"=== d ===\")\n",
    "print (nn.d)\n",
    "\n",
    "print (\"=== diffs ===\")\n",
    "diffw = np.max([np.max(np.abs(a-b)) for a,b in zip(nw, w)])\n",
    "diffb = np.max([np.max(np.abs(a-b)) for a,b in zip(nb, b)])\n",
    "\n",
    "if diffw < 1e-7 and diffb < 1e-7:\n",
    "    clear_output(True)\n",
    "    \n",
    "print (\"max w diff: \", diffw)\n",
    "print (\"max b diff: \", diffb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly calculation a/z in batch vs singular case\n"
     ]
    }
   ],
   "source": [
    "print (nn.ff(x_train[0, :]))\n",
    "print (\"=== z & a[0] ===\")\n",
    "print (nn.z)\n",
    "print (nn.a)\n",
    "\n",
    "z0 = nn.z\n",
    "a0 = nn.a\n",
    "\n",
    "print (nn.ff(x_train[1, :]))\n",
    "print (\"=== z & a[1] ===\")\n",
    "print (nn.z)\n",
    "print (nn.a)\n",
    "\n",
    "z1 = nn.z\n",
    "a1 = nn.a\n",
    "\n",
    "print (nn.ff(x_train))\n",
    "print (\"=== z & a ===\")\n",
    "zzz = [np.vstack([a, b]) for a,b in zip(z0, z1)]\n",
    "aaa = [np.vstack([a, b]) for a,b in zip(a0, a1)]\n",
    "print(zzz)\n",
    "print (nn.z)\n",
    "print (nn.a)\n",
    "\n",
    "for a,b in zip(nn.z, zzz):\n",
    "    print (a.shape, b.shape)\n",
    "    \n",
    "for a,b in zip(nn.a, aaa):\n",
    "    print (a.shape, b.shape)\n",
    "\n",
    "zcond = np.all([np.allclose(a,b) for a,b in zip(zzz, nn.z)])\n",
    "acond = np.all([np.allclose(a,b) for a,b in zip(aaa, nn.a)])\n",
    "\n",
    "if zcond and acond:\n",
    "    clear_output(True)\n",
    "    print (\"Correctly calculation a/z in batch vs singular case\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== d[0] ===\n",
      "[array([[-0.1616319863,  0.25899534  ,  0.0018112744]]), array([[ 0.42160573  ,  0.7786396014,  0.92496535  ]])]\n",
      "=== d[1] ===\n",
      "[array([[-0.1102168239,  0.4346289845,  0.1701502338]]), array([[-0.541404714 ,  0.7533900364,  0.9036963186]])]\n",
      "=== z & a ===\n",
      "[array([[-0.0808159932,  0.12949767  ,  0.0009056372],\n",
      "       [-0.055108412 ,  0.2173144923,  0.0850751169]]), array([[ 0.210802865 ,  0.3893198007,  0.462482675 ],\n",
      "       [-0.270702357 ,  0.3766950182,  0.4518481593]])]\n"
     ]
    }
   ],
   "source": [
    "nn.grad(x_train[[0], :], y_train[[0], :])\n",
    "print (\"=== d[0] ===\")\n",
    "print (nn.d)\n",
    "d0 = nn.d\n",
    "\n",
    "nn.grad(x_train[[1], :], y_train[[1], :])\n",
    "print (\"=== d[1] ===\")\n",
    "print (nn.d)\n",
    "d1 = nn.d\n",
    "\n",
    "nn.grad(x_train, y_train)\n",
    "print (\"=== z & a ===\")\n",
    "print (nn.d)\n",
    "ddd = [np.vstack([a, b]) for a,b in zip(d0, d1)]\n",
    "\n",
    "dcond = np.all([np.allclose(a,b) for a,b in zip(ddd, nn.d)])\n",
    "\n",
    "if dcond:\n",
    "    clear_output(True)\n",
    "    print (\"Correctly calculation d in batch vs singular case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything is fine :)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "l = 1\n",
    "\n",
    "w, b = nn.grad(x_train[[0], :], y_train[[0], :])\n",
    "w1, b1 = nn.ngrad(x_train[[0], :], y_train[[0], :])\n",
    "\n",
    "print (\"=== d[0] ===\")\n",
    "print (\"w\\n\", w[l])\n",
    "print (\"b\\n\", b[l])\n",
    "\n",
    "print(\"=== num ===\")\n",
    "print (w1[l])\n",
    "print (b1[l])\n",
    "\n",
    "w, b =nn.grad(x_train[[1], :], y_train[[1], :])\n",
    "w1, b1 =nn.ngrad(x_train[[1], :], y_train[[1], :])\n",
    "\n",
    "print (\"=== d[1] ===\")\n",
    "print (\"w\\n\", w[l])\n",
    "print (\"b\\n\", b[l])\n",
    "\n",
    "print(\"=== num ===\")\n",
    "print (w1[l])\n",
    "print (b1[l])\n",
    "\n",
    "w, b = nn.grad(x_train, y_train)\n",
    "w1, b1 =nn.ngrad(x_train, y_train)\n",
    "\n",
    "print (\"=== full ===\")\n",
    "print (\"dw\\n\", w[l])\n",
    "print (\"db\\n\", b[l])\n",
    "\n",
    "print (\"ww\\n\", nn.w[l])\n",
    "\n",
    "print(\"=== num ===\")\n",
    "print (\"dw\\n\", w1[l])\n",
    "print (\"\")\n",
    "print (\"db\\n\", b1[l])\n",
    "\n",
    "wclose = np.allclose(w[l], w1[l])\n",
    "bclose = np.allclose(b[l], b1[l])\n",
    "\n",
    "print (\"dw [all close]:\", wclose)\n",
    "print (\"db [all close]:\", bclose)\n",
    "\n",
    "if wclose and bclose:\n",
    "    clear_output(True)\n",
    "    print(\"everything is fine :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-ca974ae89b0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "\n",
    "x_train = np.random.uniform(size=(10, 1))\n",
    "y_train = np.sin(x_train)\n",
    "xy_train = np.hstack((x_train, y_train))\n",
    "\n",
    "x_cv = np.linspace(0,1,200).reshape(200, 1)\n",
    "y_cv = np.sin(x_cv)\n",
    "x_test, y_test = None, None\n",
    "\n",
    "maxEpoh = 2000\n",
    "epoh = []\n",
    "t_err = []\n",
    "cv_err = []\n",
    "nn = NN((1,3,1), sqrcost, sigmoid)\n",
    "\n",
    "print (x_train.shape)\n",
    "\n",
    "for i in trange(maxEpoh + 1):\n",
    "    nn.trainJoinedData(xy_train, 1, minibatch = 10, learn=0.3)\n",
    "    if i % 3 == 0:\n",
    "        epoh.append(i)\n",
    "        t_err.append(nn.avgCost(x_train, y_train))\n",
    "        cv_err.append(nn.avgCost(x_cv, y_cv))\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        clear_output(True)\n",
    "        f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "        \n",
    "        ax1.plot(epoh, t_err, label=\"training\")\n",
    "        ax1.plot(epoh, cv_err, label=\"cv\")\n",
    "        ax1.plot()\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.plot(epoh[-5:], t_err[-5:], label=\"training\")\n",
    "        ax2.plot(epoh[-5:], cv_err[-5:], label=\"cv\")\n",
    "        ax2.plot()\n",
    "        ax2.legend()\n",
    "        \n",
    "        \n",
    "        x = np.linspace(0,1, 50).reshape(50,1)\n",
    "        y = nn.ff(x)\n",
    "        ax3.plot(x, y, label=\"$\\sin x$\")\n",
    "        ax3.plot(x_train, y_train, 'o', label='input data')\n",
    "        ax3.legend()\n",
    "  \n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
